---
title: "Test Report"
author: "Jiawen Qi"
date: "March 20, 2017"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Conceptual Question

## a

**Whether the appropriate method would be classification or regression?**    
We should use regression method, because 'Salary' is a continuous variable.

**Whether we are most interested in inference or prediction?**    
We are most interested in inference, because we are trying to explain and interpret observations based on the evidence.

**Indidate what n and p are for each section**    
Response variable: salary of the CEO    
Predictor variables: industry, number of employees, and total profit

## b

**Whether the appropriate method would be classification or regression?**  
It's a classification problem, because there are two class of the product: success or failure.

**Whether we are most interested in inference or prediction?**  
Prediction, because we are trying to guess about a future event. 

**Indidate what n and p are for each section**  
Response variable: whether or not the product succeeded or failed  
Predictor variables: price of the product, competition price, marketing budget, ten other variables

## c

**Whether the appropriate method would be classification or regression?**  
Regression, because '% change in the dollar' is continuous not categorical.

**Whether we are most interested in inference or prediction?**  
Prediction, because we are trying to predict the future '% change in the dollar', guess about a future event.

**Indidate what n and p are for each section**  
Response variable: % change in the dollar  
Predictor variables: % change in the market in the United States, the % change in the market in China, and the % change in the market in France.

# 2 Applied Question

```{r, tidy=TRUE}
cars_mileage <- read.csv("Cars_mileage.csv", header = TRUE, stringsAsFactors = FALSE) # import tha dataset
head(cars_mileage) # look at the first 6 lines to have a taste
summary(cars_mileage) # have an overview
```

## a. Create a binary variable mpg_binary

```{r, tidy=TRUE}
cars_mileage$mpg_binary <- 0
cars_mileage$mpg_binary[cars_mileage$mpg > median(cars_mileage$mpg)] <- 1
cars_mileage$mpg_binary <- as.factor(cars_mileage$mpg_binary)
```

## b. Which of the other variables seem most likely to be useful in predicting whether a car's mpg is above or below its median? 

Before deciding which variable seems most likely to be useful, we need to dig into each variable to understand it:

### Dig into each variable

```{r, tidy=TRUE}
##### cylinders #####
summary(cars_mileage$cylinders)
table(cars_mileage$cylinders)
cars_mileage$cylinders <- as.factor(cars_mileage$cylinders)
```

`cylinders` variable is the number of cylinders in a car. In this dataset, most of the cars have 4 cylinders. I would like to treat this attribute as a factor, because, you can not seperate a cynlinder, eg. 2.5 cylinders.

```{r, tidy=TRUE}
##### displacement #####
summary(cars_mileage$displacement)
table(cars_mileage$displacement)
```

`displacement`variable ranges from 68 to 455. It's a continuous variable.

```{r, tidy=TRUE}
##### horsepower #####
summary(cars_mileage$horsepower)
table(cars_mileage$horsepower)
##### Imputing missing value #####
cars_mileage$horsepower.new = cars_mileage$horsepower
cars_mileage$horsepower.new[cars_mileage$horsepower.new == '?'] <- NA
cars_mileage$horsepower.new <- as.integer(cars_mileage$horsepower.new)
library(Hmisc)
cars_mileage$imputed_horsepower <- as.integer(with(cars_mileage, impute(horsepower.new, mean)))
summary(cars_mileage$imputed_horsepower)
table(cars_mileage$imputed_horsepower)
```

Why `horsepower` variables is treated as character? Because there are 5 missing value marked as ? not `NA`. This is so tricky here. We cannot find this kind of missing value by function `is.na()`. Therefore, I imputed the missing value using `Hmisc`. After imputing, this variable rangers from 46 to 230.

```{r, tidy=TRUE}
##### weight #####
summary(cars_mileage$weight)
```

`weight` ranges from 1613 to 5140. 

```{r, tidy=TRUE}
##### acceleration #####
summary(cars_mileage$acceleration)
```

`acceleration` ranges from 8 to 24.8. 

```{r, tidy=TRUE}
##### year #####
summary(cars_mileage$year)
table(cars_mileage$year)
cars_mileage$age <- 117- cars_mileage$year
```

`year` is treated as integer in this dataset. I would like to calculate the `age` of a car.

```{r, tidy=TRUE}
##### origin #####
summary(cars_mileage$origin)
table(cars_mileage$origin)
cars_mileage$origin <- as.factor(cars_mileage$origin)
```

Actually, I don't have the description for this dataset. All I can do is trying to guess and understand each variable. Also, I don't have much interest in cars. With my guess, the `origin` here means first hand, second hand, third hand? I'm not sure, maybe other meaning, but I think it's a categotical variable.

```{r, tidy=TRUE}
##### name #####
summary(cars_mileage$name)
cars_mileage$brands <- gsub( " .*$", "", cars_mileage$name)
table(cars_mileage$brands)
cars_mileage$brands[cars_mileage$brands == 'capri'] <- 'mercury'
cars_mileage$brands[cars_mileage$brands == 'chevroelt'] <- 'chevrolet'
cars_mileage$brands[cars_mileage$brands == 'chevy'] <- 'chevrolet'
cars_mileage$brands[cars_mileage$brands == 'maxda'] <- 'mazda'
cars_mileage$brands[cars_mileage$brands == 'mercedes'] <- 'mercedes-benz'
cars_mileage$brands[cars_mileage$brands == 'toyouta'] <- 'toyota'
cars_mileage$brands[cars_mileage$brands == 'vokswagen'] <- 'volkswagen'
cars_mileage$brands[cars_mileage$brands == 'vw'] <- 'volkswagen'
table(cars_mileage$brands)
cars_mileage$brands <- as.factor(cars_mileage$brands)
```

`name` is a character variable. I don't know the exact type or series for each car, but at least, I know the brands! We can extract the brand for each car. After aggregate the brands, there are several interesting points:

- "capri" is not the brand of a car, it belongs to brand mercury. <https://en.wikipedia.org/wiki/Mercury_Capri#Capri_II>  
- "chevroelt" or "chevrolet": actually on the internet, I can only find the chevrolet, so it should be a typo.  
- "chevy": when I searched this brand in Google, it cames up information of chevrolet.
- "hi": I cannot find this brand on the internet. So let's keep it.
- "maxda" or "mazda": after searching, I think it's a typo, should be mazda
- "mercedes"" or "mercedes-benz": actually the full name is mercedes-benz
- "toyota" or "toyouta": another typo, should be toyota 
- "vokswagen" or "volkswagen": another typo, should be volkswagen 
- "vw", is the abbr of volkswagen

### Correlation Matrix

Now, the dataset is:

```{r, tidy=TRUE}
dataset <- cars_mileage[,c(10, 2, 3, 5, 6, 8, 12:14)]
names(dataset)
colnames(dataset) <- c("mpg_binary", "cyl", "dis", "wei", "acc", "ori", "hor", "age", "bra")
```

Make the plot:

```{r, tidy=TRUE}
library(corrplot)
dataset.num <- lapply(dataset, function(x) as.numeric(as.character(x)))## brands are changed into NA
dataset.num$bra <- as.numeric(dataset$bra)
dataset.num <- as.data.frame(dataset.num)
M <- cor(dataset.num)
corrplot.mixed(M)
```

### Which variable seems most likely to be useful?

Weight it the variable seems most likely to be useful. Second, are cylinders and displacement.

## c. Split the data into a training set and a test set.

I would prefer 80% training and 20% testing. (There are too many levels in brands, we will not include this feature)

```{r, tidy=TRUE}
set.seed(66666) # my favorite seed
trainingIndex <- sample(nrow(dataset)*0.8)
testingIndex <- setdiff(seq(1, nrow(dataset)), trainingIndex)
training <- dataset[trainingIndex, -9]
testing <- dataset[testingIndex, -9]
```

## d. Perform two of the following in order to predict mpg_binary:

### Logistic Regression

```{r, tidy=TRUE}
LGmodel <- glm(mpg_binary ~.,family=binomial(link='logit'), data=training)
summary(LGmodel)
fitted.lg <- predict(LGmodel,newdata=testing, type='response')
fitted.lg <- ifelse(fitted.lg > 0.5, 1, 0)
confusion.matrix <- table(testing$mpg_binary, fitted.lg)
accuracy <- (confusion.matrix[1,1]+confusion.matrix[2,2])/sum(confusion.matrix)
error <- 1 - accuracy
recall <- confusion.matrix[1,1]/sum(confusion.matrix[1,])
precision <- confusion.matrix[1,1]/sum(confusion.matrix[,1])
f1 <- 2*confusion.matrix[1,1]/(2*confusion.matrix[1,1]+confusion.matrix[1,2]+confusion.matrix[2,1])
paste("accuracy:", accuracy,
      "error:", error,
      "precision:", precision,
      "recall:", recall,
      "f1score:", f1)
```

When using logistic regression to do the model, weight and age are two significant variables with negative coefficients. This is really understandable, because old and heavy car will consume more energy and the mpg will be lower, which means it's a gas guzzler. When a car is younger and lighter, it is more economical and fuel-efficient. The test error for this model is 0.0875.

### Random Forest

```{r, tidy=TRUE}
library(randomForest)
library(reprtree)
set.seed(66666)
modelRF <- randomForest(mpg_binary ~ ., data=training, importance=TRUE, ntree = 2000)
print(modelRF) 
varImpPlot(modelRF)
reprtree:::plot.getTree(modelRF)
prediction <- predict(modelRF, testing)
confusion.matrix <- table(testing$mpg_binary, prediction)
accuracy <- (confusion.matrix[1,1]+confusion.matrix[2,2])/sum(confusion.matrix)
error <- 1 - accuracy
recall <- confusion.matrix[1,1]/sum(confusion.matrix[1,])
precision <- confusion.matrix[1,1]/sum(confusion.matrix[,1])
f1 <- 2*confusion.matrix[1,1]/(2*confusion.matrix[1,1]+confusion.matrix[1,2]+confusion.matrix[2,1])
paste("accuracy:", accuracy,
      "error:", error,
      "precision:", precision,
      "recall:", recall,
      "f1score:", f1)
```

In the modeling, displacement goes at the top of the tree. The accuracy is 0.8875, and the error is 0.1125.